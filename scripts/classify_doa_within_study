# Train and test a classifier based on the within- and between- parcel rsfc across UO_TDS, OHSU_TSS, and OHSU_MINA studies
# T Cheng | 9/10/2019

import csv
import pyreadr
import numpy as np
import statistics as stats
from random import sample

from sklearn.model_selection import LeaveOneOut, train_test_split, GridSearchCV, cross_val_score
from sklearn.utils import Bunch

loo = LeaveOneOut()
from sklearn.svm import LinearSVC, SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.datasets import base
from sklearn.feature_selection import SelectPercentile, f_classif

import matplotlib.pyplot as plt
import pandas as pd

from sklearn.pipeline import make_pipeline
run_permutation = False # this takes ~ 1 hr
run_sampling = True

### LOAD DATA ###

# Directory and file names
working_dir = '/Users/theresacheng/projects/doa_classification/'
subject_list_path = working_dir + 'data/inc_subj_full_ids.csv'
beh_path = working_dir + 'data/beh_K_study.RDS'
comm_conns_path = working_dir + 'data/comm_cors_z_K_study.csv' # re-run with data Z-transformed within study (or: comm_cors_z_TDS.csv)

# Load and clean subject id list, generate study and full_subject_ids vectors
with open(subject_list_path, newline='') as subject_id_file:
    reader = csv.reader(subject_id_file)
    full_subject_ids_raw = list(reader)

study = []
full_subject_ids = []

for a, b in full_subject_ids_raw[1:len(full_subject_ids_raw)]:
    study.append(a)
    full_subject_ids.append(b)

# Load behavioral data
beh = pyreadr.read_r(beh_path)[None]

# Load within and between community connectivity rsfc MRI data
comm_conns_file = open(comm_conns_path, "r+")
comm_conns = list(csv.reader(comm_conns_file, delimiter='\t'))

# Create dataframe for adversity classification
target_names = 'maltreatment'
feature_names = list(comm_conns[0])[0].split(",")[1:92] # make a list of the column names split by commas, remove subject_id
raw_data = list(comm_conns[1:len(comm_conns)])

data = []
for row in raw_data:
    row_as_list_subj_id = row[0].split(",")  # there are 92 items per row
    row_as_list = row_as_list_subj_id[1:92] # remove subject_id
    row_as_list = list(map(float, row_as_list))
    data.append(row_as_list)
data = np.asarray(data)

target = np.asarray(beh.maltreatment)

classify_adv_df = base.Bunch(target_names=target_names,
                             feature_names=feature_names,
                             target=target,
                             data=data)

# Train and test adversity classifier

# Specify the hyperparameter space, test code here:
# parameters = {'SVM__C':[1, 10, 100],
#               'SVM__gamma':[0.1, 0.01]}
#
# c_space = np.logspace(-5, 8, 15)
# param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}
# temp = GridSearchCV(svc, param_grid, cv = 5)

# Create splits with leave-one-out cross validation
loo.get_n_splits(classify_adv_df.data)

# fit models
svc = LinearSVC()
percent_of_features = 25
feat_sel = SelectPercentile(f_classif, percent_of_features)

y_pred_all = []
y_test_all = []
n_features = round((percent_of_features/100)*len(classify_adv_df.data[0]))
feat_sel_idxes = np.empty((len(classify_adv_df.data), n_features), dtype=np.int8) #(

for train_index, test_index in loo.split(classify_adv_df.data):
   # print("TRAIN:", train_index, "TEST:", test_index)

    # generate train and test feature and outcome sets
    X_train, X_test = classify_adv_df.data[train_index], classify_adv_df.data[test_index]
    y_train, y_test = classify_adv_df.target[train_index], classify_adv_df.target[test_index]

    # apply feature selection using the training group only
    feat_sel.fit(X_train, y_train) # fit ANOVA models, ID top 15
    X_train_new = feat_sel.transform(X_train) # pare down training set features
    X_test_new = feat_sel.transform(X_test)  # pare down test set features
    #feat_sel_idxes[test_index] = feat_sel.get_support(indices=True)

    # fit model with pared down training set
    svc.fit(X_train_new, y_train)
    y_pred_all.append(svc.predict(X_test_new)) # predict outcomes based on new test set
    y_test_all.append(y_test)

adv_tn, adv_fp, adv_fn, adv_tp = confusion_matrix(y_test_all, y_pred_all).ravel()
pred_adv_accuracy = (adv_tn + adv_tp)/(adv_tn + adv_tp + adv_fn + adv_fp)
print('Adversity status accuracy:', round(pred_adv_accuracy, 2))

# plt.hist(feat_sel_idxes, bins=91)

# Conduct control analyses
all_adv_accuracies = []

# re-run classifier 1000x with shuffled target labels and save accuracy values
if run_permutation == True:
    for i in range(1000):
        idx = np.random.permutation(len(classify_adv_df.target))
        y = classify_adv_df.target[idx]

        shuffled_adv_df = classify_adv_df
        shuffled_adv_df.target = y

        y_pred_all = []
        y_test_all = []

        for train_index, test_index in loo.split(shuffled_adv_df.data):
            print("TRAIN:", train_index, "TEST:", test_index)
            X_train, X_test = shuffled_adv_df.data[train_index], shuffled_adv_df.data[test_index]
            y_train, y_test = shuffled_adv_df.target[train_index], shuffled_adv_df.target[test_index]

            # apply feature selection using the training group only
            feat_sel.fit(X_train, y_train)  # fit ANOVA models, ID top 15
            X_train_new = feat_sel.transform(X_train)  # pare down training set features
            X_test_new = feat_sel.transform(X_test)  # pare down test set features
            # feat_sel_idxes[test_index] = feat_sel.get_support(indices=True)

            svc.fit(X_train_new, y_train)
            y_pred_all.append(svc.predict(X_test_new))
            y_test_all.append(y_test)

        adv_tn, adv_fp, adv_fn, adv_tp = confusion_matrix(y_test_all, y_pred_all).ravel()
        pred_adv_accuracy = (adv_tn + adv_tp)/(adv_tn + adv_tp + adv_fn + adv_fp)

        all_adv_accuracies.append(pred_adv_accuracy)

    # plot a distribution of accuracy values
    plt.hist(all_adv_accuracies, bins='auto')
    plt.savefig('output/all_adv_perm_accuracies.png')

    plt.close()

    all_adv_accuracies_quartiles = [round(np.percentile(all_adv_accuracies, 25), 2),
                                    round(np.percentile(all_adv_accuracies, 50), 2),
                                    round(np.percentile(all_adv_accuracies, 75), 2),
                                    round(np.percentile(all_adv_accuracies, 95), 2)]

    print('Permuted adversity accuracy 1st, 2nd, 3rd quartile and 95%:', all_adv_accuracies_quartiles)

### Exposure-specific analyses ###
# Create df for exposure type analysis

# generate indices based on exposure status
# note that these indices intentionally exclude participants with adversity but who don't endorse a specific type
abuse_only_idx = np.where(np.logical_and(np.logical_and(beh.abuse.astype(bool) == True, beh.neglect.astype(bool) == False), beh.maltreatment.astype(bool) == True))
abuse_only_idx = np.asarray(abuse_only_idx)[0]

neglect_only_idx = np.where(np.logical_and(np.logical_and(beh.abuse.astype(bool) == False, beh.neglect.astype(bool) == True), beh.maltreatment.astype(bool) == True))
neglect_only_idx = np.asarray(neglect_only_idx)[0]

both_idx = np.where(np.logical_and(np.logical_and(beh.abuse.astype(bool) == True, beh.neglect.astype(bool) == True), beh.maltreatment.astype(bool) == True))
both_idx = np.asarray(both_idx)[0]

none_idx = np.where(np.logical_and(np.logical_and(beh.abuse.astype(bool) == False, beh.neglect.astype(bool) == False), beh.maltreatment.astype(bool) == False))
none_idx = np.asarray(none_idx)[0]

# convert pandas dataframe into numpy array, and then delete and reconstitute
np_beh_values = beh.values

# re-run classifier 1000x classifying abuse against a subset of none individuals, save accuracy values
all_abuse_accuracies = []

if run_sampling == True:
    for i in range(1000):
        # take a random subset of the none_idx so that they match the number of individuals that have experienced abuse
        none_idx_subsample = sample(list(none_idx), len(abuse_only_idx))

        # Train and test abuse classifier with loo
        ## abuse vs. none
        abuse_none_beh_values = np_beh_values[np.sort(np.concatenate([abuse_only_idx, none_idx_subsample]))]
        abuse_none_beh = pd.DataFrame(abuse_none_beh_values, columns = beh.columns)
        classify_abuse_df = base.Bunch(target_names='abuse',
                                       feature_names=feature_names,
                                       target=np.asarray(abuse_none_beh.abuse.astype(int)),
                                       data=data[np.sort(np.concatenate([abuse_only_idx, none_idx_subsample]))])

        y_pred_abuse = []
        y_test_abuse = []

        loo.get_n_splits(classify_abuse_df.data)

        for train_index, test_index in loo.split(classify_abuse_df.data):
            #print("TRAIN:", train_index, "TEST:", test_index)

            # generate train and test feature and outcome sets
            X_train, X_test = classify_abuse_df.data[train_index], classify_abuse_df.data[test_index]
            y_train, y_test = classify_abuse_df.target[train_index], classify_abuse_df.target[test_index]

            # apply feature selection using the training group only
            feat_sel.fit(X_train, y_train)  # fit ANOVA models, ID top 15%
            X_train_new = feat_sel.transform(X_train)  # pare down training set features
            X_test_new = feat_sel.transform(X_test)  # pare down test set features
            #feat_sel_idxes[test_index] = feat_sel.get_support(indices=True)

            # fit model with pared down training set
            svc.fit(X_train_new, y_train)
            y_pred_abuse.append(svc.predict(X_test_new))  # predict outcomes based on new test set
            y_test_abuse.append(y_test)

        abuse_tn, abuse_fp, abuse_fn, abuse_tp = confusion_matrix(y_test_abuse, y_pred_abuse).ravel()
        pred_abuse_accuracy = (abuse_tn + abuse_tp)/(abuse_tn + abuse_tp + abuse_fn + abuse_fp)

        all_abuse_accuracies.append(pred_abuse_accuracy)

    # plot a distribution of accuracy values
    #plt.hist(all_abuse_accuracies, bins='auto')
    #plt.show()
    #plt.savefig('output/abuse_accuracies.png')
    #plt.savefig('output/abuse_accuracies_hs_feat.png')

plt.close()

print('Abuse status accuracy:', round(stats.mean(all_abuse_accuracies), 2))

# re-run classifier 1000x with shuffled target labels and save accuracy values
all_neglect_accuracies = []

if run_sampling == True:
    for i in range(1000):
        # take a random subset of the none_idx so that they match the number of individuals that have experienced abuse
        none_idx_subsample = sample(list(none_idx), len(neglect_only_idx))


        # Train and test neglect classifier with loo
        ## neglect vs. none
        neglect_none_beh_values = np_beh_values[np.sort(np.concatenate([neglect_only_idx, none_idx_subsample]))]
        neglect_none_beh = pd.DataFrame(neglect_none_beh_values, columns = beh.columns)
        classify_neglect_df = base.Bunch(target_names='neglect',
                                       feature_names=feature_names,
                                       target=np.asarray(neglect_none_beh.neglect.astype(int)),
                                       data=data[np.sort(np.concatenate([neglect_only_idx, none_idx_subsample]))])

        y_pred_neglect = []
        y_test_neglect = []

        loo.get_n_splits(classify_neglect_df.data)

        for train_index, test_index in loo.split(classify_neglect_df.data):
           # print("TRAIN:", train_index, "TEST:", test_index)
            X_train, X_test = classify_neglect_df.data[train_index], classify_neglect_df.data[test_index]
            y_train, y_test = classify_neglect_df.target[train_index], classify_neglect_df.target[test_index]

            # apply feature selection using the training group only
            feat_sel.fit(X_train, y_train)  # fit ANOVA models, ID top 15%
            X_train_new = feat_sel.transform(X_train)  # pare down training set features
            X_test_new = feat_sel.transform(X_test)  # pare down test set features
            #feat_sel_idxes[test_index] = feat_sel.get_support(indices=True)

            svc.fit(X_train_new, y_train)
            y_pred_neglect.append(svc.predict(X_test_new))
            y_test_neglect.append(y_test)

        neglect_tn, neglect_fp, neglect_fn, neglect_tp = confusion_matrix(y_test_neglect, y_pred_neglect).ravel()
        pred_neglect_accuracy = (neglect_tn + neglect_tp)/(neglect_tn + neglect_tp + neglect_fn + neglect_fp)

        all_neglect_accuracies.append(pred_neglect_accuracy)

    # plot a distribution of accuracy values
    #plt.hist(all_neglect_accuracies, bins='auto')
    #plt.show()
    #plt.savefig('output/neglect_accuracies.png')
    #plt.savefig('output/neglect_accuracies_hs_feat.png')

#plt.close()

print('Neglect status accuracy:', round(stats.mean(all_neglect_accuracies), 2))


all_both_accuracies = []

if run_sampling == True:
    for i in range(1000):
        # take a random subset of the none_idx so that they match the number of individuals that have experienced abuse
        none_idx_subsample = sample(list(none_idx), len(both_idx))

        # Train and test both classifier with loo
        ## both vs. none
        both_none_beh_values = np_beh_values[np.sort(np.concatenate([both_idx, none_idx_subsample]))]
        both_none_beh = pd.DataFrame(both_none_beh_values, columns = beh.columns)
        classify_both_df = base.Bunch(target_names='both',
                                       feature_names=feature_names,
                                       target=np.asarray(both_none_beh.polyvictimization.astype(int)),
                                       data=data[np.sort(np.concatenate([both_idx, none_idx_subsample]))])

        y_pred_both = []
        y_test_both = []

        loo.get_n_splits(classify_both_df.data)

        for train_index, test_index in loo.split(classify_both_df.data):
            #print("TRAIN:", train_index, "TEST:", test_index)
            X_train, X_test = classify_both_df.data[train_index], classify_both_df.data[test_index]
            y_train, y_test = classify_both_df.target[train_index], classify_both_df.target[test_index]

            # apply feature selection using the training group only
            feat_sel.fit(X_train, y_train)  # fit ANOVA models, ID top 15%
            X_train_new = feat_sel.transform(X_train)  # pare down training set features
            X_test_new = feat_sel.transform(X_test)  # pare down test set features
            #feat_sel_idxes[test_index] = feat_sel.get_support(indices=True)

            svc.fit(X_train_new, y_train)
            y_pred_both.append(svc.predict(X_test_new))
            y_test_both.append(y_test)

        both_tn, both_fp, both_fn, both_tp = confusion_matrix(y_test_both, y_pred_both).ravel()
        pred_both_accuracy = (both_tn + both_tp)/(both_tn + both_tp + both_fn + both_fp)

        all_both_accuracies.append(pred_both_accuracy)

    # plot a distribution of accuracy values
    #plt.hist(all_both_accuracies, bins='auto')
    #plt.show()
    #plt.savefig('output/both_accuracies.png')
    #plt.savefig('output/both_accuracies_hs_feat.png')

plt.close()

print('Both status accuracy:', round(stats.mean(all_both_accuracies), 2))